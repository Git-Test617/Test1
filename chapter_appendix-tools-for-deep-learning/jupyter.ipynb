{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOgZhYlZ3vof",
        "outputId": "31e8e659-b52f-4441-c0fe-a33b7cabbf94"
      },
      "id": "MOgZhYlZ3vof",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FAKE_PATH = '/content/drive/MyDrive/Minor Project 7th Sem Dataset /Fake.csv'   # <-- change if your path differs\n",
        "TRUE_PATH = '/content/drive/MyDrive/Minor Project 7th Sem Dataset /True.csv'   # <-- change if your path differs\n",
        "\n",
        "# Quick check to see files exist\n",
        "import os\n",
        "print(\"Fake exists:\", os.path.exists(FAKE_PATH))\n",
        "print(\"True exists:\", os.path.exists(TRUE_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbKIcvLv4Hw5",
        "outputId": "acf1a857-46b3-45cc-9c0d-6685d600b806"
      },
      "id": "NbKIcvLv4Hw5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake exists: False\n",
            "True exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 — Install PyG (PyTorch Geometric) and other libs (run once)\n",
        "# These installs are a bit heavy but required for GNNs. If you prefer not to install, skip GNN cells.\n",
        "!pip install -q torch==2.2.2+cu121 torchvision==0.15.2+cu121 torchaudio==2.2.2 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q scikit-learn nltk umap-learn\n",
        "# PyG might already be available sometimes; installs can take a couple minutes.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6fon_WX4LZr",
        "outputId": "1db5a93d-cfd0-44a4-9a17-7d38d2e71a9b"
      },
      "id": "X6fon_WX4LZr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.15.2+cu121 (from versions: 0.1.6, 0.2.0, 0.17.0, 0.17.0+cu121, 0.17.1, 0.17.1+cu121, 0.17.2, 0.17.2+cu121, 0.18.0, 0.18.0+cu121, 0.18.1, 0.18.1+cu121, 0.19.0, 0.19.0+cu121, 0.19.1, 0.19.1+cu121, 0.20.0, 0.20.0+cu121, 0.20.1, 0.20.1+cu121, 0.21.0, 0.22.0, 0.22.1, 0.23.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.15.2+cu121\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 — Imports & lightweight NLP setup\n",
        "import os, re, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.kernel_approximation import RBFSampler   # Random Fourier Features (RFF)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqDqaO5H4O_W",
        "outputId": "af60132a-8265-4cc7-d175-fa51ac5997c2"
      },
      "id": "yqDqaO5H4O_W",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — Load CSVs and quick combine\n",
        "df_fake = pd.read_csv(FAKE_PATH)\n",
        "df_true = pd.read_csv(TRUE_PATH)\n",
        "\n",
        "df_fake['label'] = 1\n",
        "df_true['label'] = 0\n",
        "df = pd.concat([df_fake, df_true], ignore_index=True).reset_index(drop=True)\n",
        "print(\"Total articles:\", len(df))\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
        "df['content'] = (df['title'].fillna('') + ' ' + df['text'].fillna('')).astype(str)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "hCE0bk5F4SnY",
        "outputId": "9687f1a1-b867-4654-fe63-44bee5330104"
      },
      "id": "hCE0bk5F4SnY",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Minor Project 7th Sem Dataset /Fake.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2933664061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 4 — Load CSVs and quick combine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFAKE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRUE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_fake\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Minor Project 7th Sem Dataset /Fake.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — Lightweight preprocess (fast)\n",
        "import re\n",
        "def preprocess_text_simple(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply to a subset if dataset huge to save time — but default apply to all\n",
        "df['clean'] = df['content'].map(preprocess_text_simple)\n"
      ],
      "metadata": {
        "id": "-1FrsxWc4YQ1"
      },
      "id": "-1FrsxWc4YQ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — Create TF-IDF features + compress with SVD to get node feature vectors (small dims)\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000, min_df=3, sublinear_tf=True)\n",
        "X_tfidf = tfidf.fit_transform(df['clean'])\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
        "\n",
        "# Compress to low-dim node features (to save memory for GNN)\n",
        "svd = TruncatedSVD(n_components=128, random_state=42)   # 64-128 dims are usually OK\n",
        "X_embed = svd.fit_transform(X_tfidf)\n",
        "print(\"Compressed features shape:\", X_embed.shape)\n"
      ],
      "metadata": {
        "id": "yZIxHEU34f78"
      },
      "id": "yZIxHEU34f78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — Build k-NN graph among documents (sparse edges). Use cosine/Euclidean on embeddings.\n",
        "k = 8   # neighbors — tune small for memory\n",
        "nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine', n_jobs=2).fit(X_embed)\n",
        "distances, indices = nbrs.kneighbors(X_embed)\n",
        "\n",
        "# Build edge_index for PyG (bidirectional edges)\n",
        "edge_index_list = []\n",
        "N = X_embed.shape[0]\n",
        "for i in range(N):\n",
        "    for j in indices[i][1:]:  # skip self (first neighbor)\n",
        "        edge_index_list.append([i, j])\n",
        "        edge_index_list.append([j, i])\n",
        "edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "print(\"Number of edges:\", edge_index.size(1))\n"
      ],
      "metadata": {
        "id": "kB7u8f1h4i3O"
      },
      "id": "kB7u8f1h4i3O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — Prepare PyG Data object for node classification\n",
        "x = torch.tensor(X_embed, dtype=torch.float)\n",
        "y = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "data = Data(x=x, edge_index=edge_index, y=y).to(device)\n"
      ],
      "metadata": {
        "id": "Nv8_kfQY4lzj"
      },
      "id": "Nv8_kfQY4lzj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 — Train / test masks (stratified split) for node classification\n",
        "train_idx, test_idx = train_test_split(np.arange(N), test_size=0.2, stratify=df['label'].values, random_state=42)\n",
        "# Optional validation split:\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.15, stratify=df['label'].values[train_idx], random_state=42)\n",
        "\n",
        "train_mask = torch.zeros(N, dtype=torch.bool)\n",
        "val_mask = torch.zeros(N, dtype=torch.bool)\n",
        "test_mask = torch.zeros(N, dtype=torch.bool)\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask.to(device)\n",
        "data.val_mask = val_mask.to(device)\n",
        "data.test_mask = test_mask.to(device)\n",
        "print(train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n"
      ],
      "metadata": {
        "id": "eZv_vTh14mi3"
      },
      "id": "eZv_vTh14mi3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 — Define a lightweight GCN model (baseline)\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=128, num_classes=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.lin = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = dropout\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        if batch is None:\n",
        "            # node classification: no pooling\n",
        "            out = self.lin(x)\n",
        "            return out, x\n",
        "        else:\n",
        "            # graph-level (not used here)\n",
        "            x = global_mean_pool(x, batch)\n",
        "            out = self.lin(x)\n",
        "            return out, x\n",
        "\n",
        "# utility train / eval\n",
        "def train_epoch(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out, _ = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask].long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def eval_model(model, data, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(data.x, data.edge_index)\n",
        "        preds = logits[mask].argmax(dim=1).cpu().numpy()\n",
        "        labels = data.y[mask].cpu().numpy()\n",
        "    return preds, labels\n"
      ],
      "metadata": {
        "id": "oqlRq-P74tIT"
      },
      "id": "oqlRq-P74tIT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 — Train baseline GCN quickly\n",
        "in_dim = data.num_node_features\n",
        "gcn = SimpleGCN(in_dim, hidden_dim=128, num_classes=2, dropout=0.5).to(device)\n",
        "optimizer = optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 10\n",
        "cur_pat = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, 101):   # small number of epochs\n",
        "    loss = train_epoch(gcn, data, optimizer, criterion)\n",
        "    val_preds, val_labels = eval_model(gcn, data, data.val_mask)\n",
        "    val_f1 = f1_score(val_labels, val_preds)\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_state = gcn.state_dict()\n",
        "        cur_pat = 0\n",
        "    else:\n",
        "        cur_pat += 1\n",
        "    if epoch % 10 == 0 or epoch==1:\n",
        "        print(f\"Epoch {epoch} loss {loss:.4f} val_f1 {val_f1:.4f}\")\n",
        "    if cur_pat >= patience:\n",
        "        print(\"Early stopping.\")\n",
        "        break\n",
        "\n",
        "# load best\n",
        "if best_state is not None:\n",
        "    gcn.load_state_dict(best_state)\n"
      ],
      "metadata": {
        "id": "7ag1ukrD4wwB"
      },
      "id": "7ag1ukrD4wwB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 — Evaluate GCN baseline on test set\n",
        "gcn.eval()\n",
        "test_preds, test_labels = eval_model(gcn, data, data.test_mask)\n",
        "print(\"GCN Test — acc {:.4f} precision {:.4f} recall {:.4f} f1 {:.4f}\".format(\n",
        "    accuracy_score(test_labels, test_preds),\n",
        "    precision_score(test_labels, test_preds),\n",
        "    recall_score(test_labels, test_preds),\n",
        "    f1_score(test_labels, test_preds)\n",
        "))\n",
        "print(classification_report(test_labels, test_preds, target_names=['Real','Fake']))\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "print(\"Confusion matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "xowgAM5n40bb"
      },
      "id": "xowgAM5n40bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 13 — Stable-GNN Training\n",
        "\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# === Helper: compute Random Fourier Features ===\n",
        "def compute_rff_features(X_np, gamma=1.0, n_components=128, random_state=42):\n",
        "    rff = RBFSampler(gamma=gamma, n_components=n_components, random_state=random_state)\n",
        "    Z = rff.fit_transform(X_np)   # shape: N x n_components\n",
        "    return Z\n",
        "\n",
        "# === Helper: compute weighted off-diagonal covariance norm ===\n",
        "def weighted_offdiag_cov_norm(Z, W):\n",
        "    W = W.reshape(-1,1)\n",
        "    W_sum = W.sum()\n",
        "    mean = (W * Z).sum(axis=0) / W_sum\n",
        "    Zc = Z - mean\n",
        "    Zw = Zc * np.sqrt(W)\n",
        "    cov = (Zw.T @ Zw) / (W_sum - 1.0)\n",
        "    offdiag = cov - np.diag(np.diag(cov))\n",
        "    return np.linalg.norm(offdiag, ord='fro')\n",
        "\n",
        "# === Simplified sample weight optimization ===\n",
        "def optimize_sample_weights(Z, init_w=None, lr=0.5, n_iters=10):\n",
        "    N = Z.shape[0]\n",
        "    if init_w is None:\n",
        "        w = np.ones(N)\n",
        "    else:\n",
        "        w = init_w.copy()\n",
        "\n",
        "    def project(w):\n",
        "        w = np.maximum(w, 1e-6)\n",
        "        return w * (N / w.sum())\n",
        "\n",
        "    w = project(w)\n",
        "    for it in range(n_iters):\n",
        "        base = weighted_offdiag_cov_norm(Z, w)\n",
        "        idxs = np.random.choice(N, size=min(200, N), replace=False)\n",
        "        for i in idxs:\n",
        "            w_pert = w.copy()\n",
        "            w_pert[i] += 1e-3\n",
        "            val = weighted_offdiag_cov_norm(Z, w_pert)\n",
        "            grad = (val - base) / 1e-3\n",
        "            w[i] -= lr * grad\n",
        "        w = project(w)\n",
        "        if it % 5 == 0:\n",
        "            print(f\"Iter {it} offdiag={base:.5f}\")\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "Z9lPRHB_44c9"
      },
      "id": "Z9lPRHB_44c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 14 — Train Stable-GNN\n",
        "\n",
        "def train_stable_gnn(data, base_model, rounds=2, rff_dim=128):\n",
        "    model = base_model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "    N = data.num_nodes\n",
        "    W = np.ones(N)\n",
        "\n",
        "    for r in range(rounds):\n",
        "        # 1) Extract node embeddings\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            _, latent = model(data.x, data.edge_index)\n",
        "            latent_np = latent.detach().cpu().numpy()\n",
        "\n",
        "        # 2) Compute RFF features\n",
        "        Z = compute_rff_features(latent_np, gamma=1.0, n_components=rff_dim)\n",
        "\n",
        "        # 3) Optimize sample weights\n",
        "        print(f\"Stable-GNN Round {r+1}\")\n",
        "        W = optimize_sample_weights(Z, init_w=W, n_iters=10)\n",
        "\n",
        "        # 4) Train with weighted loss\n",
        "        model.train()\n",
        "        for epoch in range(5):\n",
        "            optimizer.zero_grad()\n",
        "            out, _ = model(data.x, data.edge_index)\n",
        "            losses = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "            train_idx_np = data.train_mask.cpu().numpy().nonzero()[0]\n",
        "            w_train = torch.tensor(W[train_idx_np] / W[train_idx_np].sum() * len(train_idx_np),\n",
        "                                   dtype=torch.float, device=device)\n",
        "            weighted_loss = (losses * w_train).mean()\n",
        "            weighted_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run Stable-GNN\n",
        "stable_gcn = SimpleGCN(data.num_node_features, hidden_dim=128, num_classes=2, dropout=0.5)\n",
        "stable_gcn = train_stable_gnn(data, stable_gcn, rounds=2, rff_dim=128)\n"
      ],
      "metadata": {
        "id": "K0JDuuZr45iO"
      },
      "id": "K0JDuuZr45iO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 15 — Evaluate Stable-GNN\n",
        "\n",
        "stable_gcn.eval()\n",
        "stable_preds, stable_labels = eval_model(stable_gcn, data, data.test_mask)\n",
        "\n",
        "print(\"Stable-GNN Test — acc {:.4f} precision {:.4f} recall {:.4f} f1 {:.4f}\".format(\n",
        "    accuracy_score(stable_labels, stable_preds),\n",
        "    precision_score(stable_labels, stable_preds),\n",
        "    recall_score(stable_labels, stable_preds),\n",
        "    f1_score(stable_labels, stable_preds)\n",
        "))\n",
        "print(classification_report(stable_labels, stable_preds, target_names=['Real','Fake']))\n",
        "\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(stable_labels, stable_preds))\n"
      ],
      "metadata": {
        "id": "LDE-6WJ_5D3Z"
      },
      "id": "LDE-6WJ_5D3Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 16 — Interactive Prediction\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a fast Logistic Regression baseline for interactive use\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=20000, sublinear_tf=True)\n",
        "X_tfidf = tfidf.fit_transform(df['clean'])\n",
        "y = df['label'].values\n",
        "\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced')\n",
        "lr.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "print(\"Interactive LR baseline ready.\")\n",
        "\n",
        "# === Function for user input ===\n",
        "def predict_news(text):\n",
        "    clean = preprocess_text_simple(text)\n",
        "    Xv = tfidf.transform([clean])\n",
        "    pred = lr.predict(Xv)[0]\n",
        "    label = \"Fake\" if pred == 1 else \"Real\"\n",
        "    prob = lr.predict_proba(Xv)[0][1]\n",
        "    return f\"Prediction: {label} (Fake prob={prob:.2f})\"\n",
        "\n",
        "# === Ask user for input ===\n",
        "while True:\n",
        "    user_input = input(\"Enter news text or headline (or 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    print(predict_news(user_input))\n"
      ],
      "metadata": {
        "id": "Gml4Est15HEX"
      },
      "id": "Gml4Est15HEX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}